@article{Goel2021,
   abstract = {Accurate identification of siblings through face recognition is a challenging task. This is predominantly because of the high degree of similarities among the faces of siblings. In this study, we investigate the use of state-of-the-art deep learning face recognition models to evaluate their capacity for discrimination between sibling faces using various similarity indices. The specific models examined for this purpose are FaceNet, VGGFace, VGG16, and VGG19. For each pair of images provided, the embeddings have been calculated using the chosen deep learning model. Five standard similarity measures, namely, cosine similarity, Euclidean distance, structured similarity, Manhattan distance, and Minkowski distance, are used to classify images looking for their identity on the threshold defined for each of the similarity measures. The accuracy, precision, and misclassification rate of each model are calculated using standard confusion matrices. Four different experimental datasets for full-frontal-face, eyes, nose, and forehead of sibling pairs are constructed using publicly available HQf subset of the SiblingDB database. The experimental results show that the accuracy of the chosen deep learning models to distinguish siblings based on the full-frontal-face and cropped face areas vary based on the face area compared. It is observed that VGGFace is best while comparing the full-frontal-face and eyes—the accuracy of classification being with more than 95% in this case. However, its accuracy degrades significantly when the noses are compared, while FaceNet provides the best result for classification based on the nose. Similarly, VGG16 and VGG19 are not the best models for classification using the eyes, but these models provide favorable results when foreheads are compared.},
   author = {Rita Goel and Irfan Mehmood and Hassan Ugail},
   doi = {10.3390/S21155068},
   issn = {14248220},
   issue = {15},
   journal = {Sensors},
   keywords = {Face recognition,FaceNet,Sibling recognition,VGG16,VGG19,VGGFace},
   month = {8},
   pmid = {34372306},
   publisher = {MDPI AG},
   title = {A study of deep learning-based face recognition models for sibling identification},
   volume = {21},
   year = {2021},
}

@article{CombiningCNN,
   abstract = {Recently, a common starting point for solving complex unsupervised image classification tasks is to use generic features, extracted with deep Convolutional Neural Networks (CNN) pretrained on a large and versatile dataset (ImageNet). However, in most research, the CNN architecture for feature extraction is chosen arbitrarily, without justification. This paper aims at providing insight on the use of pretrained CNN features for image clustering (IC). First, extensive experiments are conducted and show that, for a given dataset, the choice of the CNN architecture for feature extraction has a huge impact on the final clustering. These experiments also demonstrate that proper extractor selection for a given IC task is difficult. To solve this issue, we propose to rephrase the IC problem as a multi-view clustering (MVC) problem that considers features extracted from different architectures as different “views” of the same data. This approach is based on the assumption that information contained in the different CNN may be complementary, even when pretrained on the same data. We then propose a multi-input neural network architecture that is trained end-to-end to solve the MVC problem effectively. This approach is tested on nine natural image datasets, and produces state-of-the-art results for IC.},
   author = {Joris Guérin and Stéphane Thiery and Eric Nyiri and Olivier Gibaru and Byron Boots},
   doi = {10.1016/J.NEUCOM.2020.10.068},
   issn = {18728286},
   journal = {Neurocomputing},
   keywords = {Image clustering,Multi-View clustering,Transfer clustering},
   month = {1},
   pages = {551-571},
   publisher = {Elsevier B.V.},
   title = {Combining pretrained CNN feature extractors to enhance clustering of complex natural images},
   volume = {423},
   year = {2021},
}

@article{IECNN,
   abstract = {In the field of face recognition, similar face recognition is difficult due to the high degree of similarity of the face structure. The following two factors are needed to make progress in this field: (i) the availability of large scale similar face training datasets, and (ii) a fine-grained face recognition method. With the above factors fulfilled, we make two contributions. First, we show how a large scale similar face dataset (SFD) can be assembled by a combination of automation and human in the loop, and divide the dataset into five grades according to different degrees of similarity. Second, a new fine-grained face feature extraction method is proposed to solve this problem using the attention mechanism which combines the Internal Features and External Features. The Labeled Faces in the Wild (LFW) database, CASIA-WebFace and similar face dataset (SFD) were selected for experiments. It turns out that the true positive rate is improved by 1.94 - 5.66% and the recognition accuracy rate improved by 2.08 - 5.8% for the LFW and CASIA-WebFace database, respectively. Meanwhile for SFD, the recognition accuracy rate improved by 18.80 - 35.84%.},
   author = {An Ping Song and Qian Hu and Xue Hai Ding and Xin Yi Di and Zi Heng Song},
   doi = {10.1109/ACCESS.2020.2978938},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Artificial neural networks,Computer vision,Face recognition,Image databases,Machine learning},
   pages = {45244-45253},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Similar Face Recognition Using the IE-CNN Model},
   volume = {8},
   year = {2020},
}

@article{Yang2018,
   abstract = {In the big data era, the data are generated from different sources or observed from different views. These data are referred to as multi-view data. Unleashing the power of knowledge in multi-view data is very important in big data mining and analysis. This calls for advanced techniques that consider the diversity of different views, while fusing these data. Multi-view Clustering (MvC) has attracted increasing attention in recent years by aiming to exploit complementary and consensus information across multiple views. This paper summarizes a large number of multi-view clustering algorithms, provides a taxonomy according to the mechanisms and principles involved, and classifies these algorithms into five categories, namely, co-training style algorithms, multi-kernel learning, multiview graph clustering, multi-view subspace clustering, and multi-task multi-view clustering. Therein, multi-view graph clustering is further categorized as graph-based, network-based, and spectral-based methods. Multi-view subspace clustering is further divided into subspace learning-based, and non-negative matrix factorization-based methods. This paper does not only introduce the mechanisms for each category of methods, but also gives a few examples for how these techniques are used. In addition, it lists some publically available multi-view datasets. Overall, this paper serves as an introductory text and survey for multi-view clustering.},
   author = {Yan Yang and Hao Wang},
   doi = {10.26599/BDMA.2018.9020003},
   issn = {20960654},
   issue = {2},
   journal = {Big Data Mining and Analytics},
   keywords = {Co-training,Graph clustering,Multi-kernel learning,Multi-task learning,Multi-view clustering,Non-negative matrix factorization,Subspace clustering,Subspace learning},
   month = {6},
   pages = {83-107},
   publisher = {Tsinghua University Press},
   title = {Multi-view clustering: A survey},
   volume = {1},
   year = {2018},
}

@article{Bodini2019,
   abstract = {The task of facial landmark extraction is fundamental in several applications which involve facial analysis, such as facial expression analysis, identity and face recognition, facial animation, and 3D face reconstruction. Taking into account the most recent advances resulting from deep-learning techniques, the performance of methods for facial landmark extraction have been substantially improved, even on in-the-wild datasets. Thus, this article presents an updated survey on facial landmark extraction on 2D images and video, focusing on methods that make use of deep-learning techniques. An analysis of many approaches comparing the performances is provided. In summary, an analysis of common datasets, challenges, and future research directions are provided.},
   author = {Matteo Bodini},
   doi = {10.3390/BDCC3010014},
   issn = {25042289},
   issue = {1},
   journal = {Big Data and Cognitive Computing},
   keywords = {Deep learning,Facial landmark extraction},
   month = {3},
   pages = {1-14},
   publisher = {MDPI AG},
   title = {A review of facial landmark extraction in 2D images and videos using deep learning},
   volume = {3},
   year = {2019},
}

@article{Wang2021,
   abstract = {Deep learning applies multiple processing layers to learn representations of data with multiple levels of feature extraction. This emerging technique has reshaped the research landscape of face recognition (FR) since 2014, launched by the breakthroughs of DeepFace and DeepID. Since then, deep learning technique, characterized by the hierarchical architecture to stitch together pixels into invariant face representation, has dramatically improved the state-of-the-art performance and fostered successful real-world applications. In this survey, we provide a comprehensive review of the recent developments on deep FR, covering broad topics on algorithm designs, databases, protocols, and application scenes. First, we summarize different network architectures and loss functions proposed in the rapid evolution of the deep FR methods. Second, the related face processing methods are categorized into two classes: “one-to-many augmentation” and “many-to-one normalization”. Then, we summarize and compare the commonly used databases for both model training and evaluation. Third, we review miscellaneous scenes in deep FR, such as cross-factor, heterogenous, multiple-media and industrial scenes. Finally, the technical challenges and several promising directions are highlighted.},
   author = {Mei Wang and Weihong Deng},
   doi = {10.1016/J.NEUCOM.2020.10.081},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {Deep face recognition,Deep learning,Deep network architecture,Face processing,Face recognition database,Loss function},
   month = {3},
   pages = {215-244},
   publisher = {Elsevier},
   title = {Deep face recognition: A survey},
   volume = {429},
   year = {2021},
}

@article{Zafeiriou2015,
   abstract = {Abstract Face detection is one of the most studied topics in computer vision literature, not only because of the challenging nature of face as an object, but also due to the countless applications that require the application of face detection as a first step. During the past 15 years, tremendous progress has been made due to the availability of data in unconstrained capture conditions (so-called 'in-the-wild') through the Internet, the effort made by the community to develop publicly available benchmarks, as well as the progress in the development of robust computer vision algorithms. In this paper, we survey the recent advances in real-world face detection techniques, beginning with the seminal Viola-Jones face detector methodology. These techniques are roughly categorized into two general schemes: rigid templates, learned mainly via boosting based methods or by the application of deep neural networks, and deformable models that describe the face by its parts. Representative methods will be described in detail, along with a few additional successful methods that we briefly go through at the end. Finally, we survey the main databases used for the evaluation of face detection algorithms and recent benchmarking efforts, and discuss the future of face detection.},
   author = {Stefanos Zafeiriou and Cha Zhang and Zhengyou Zhang},
   doi = {10.1016/J.CVIU.2015.03.015},
   issn = {1077-3142},
   journal = {Computer Vision and Image Understanding},
   keywords = {Boosting,Deep neural networks,Deformable models,Face detection,Feature extraction},
   month = {9},
   pages = {1-24},
   publisher = {Academic Press},
   title = {A survey on face detection in the wild: Past, present and future},
   volume = {138},
   year = {2015},
}

@article{FaceDetection2001,
   abstract = {In this paper we present a comprehensive and critical survey of face detection algorithms. Face detection is a necessary first-step in face recognition systems, with the purpose of localizing and extracting the face region from the background. It also has several applications in areas such as content-based image retrieval, video coding, video conferencing, crowd surveillance, and intelligent human-computer interfaces. However, it was not until recently that the face detection problem received considerable attention among researchers. The human face is a dynamic object and has a high degree of variability in its appearance, which makes face detection a difficult problem in computer vision. A wide variety of techniques have been proposed, ranging from simple edge-based algorithms to composite high-level approaches utilizing advanced pattern recognition methods. The algorithms presented in this paper are classified as either feature-based or image-based and are discussed in terms of their technical approach and performance. Due to the lack of standardized tests, we do not provide a comprehensive comparative evaluation, but in cases where results are reported on common datasets, comparisons are presented. We also give a presentation of some proposed applications and possible application areas.},
   author = {Erik Hjelmås and Boon Kee Low},
   doi = {10.1006/CVIU.2001.0921},
   issn = {10773142},
   issue = {3},
   journal = {Computer Vision and Image Understanding},
   keywords = {Face detection,Face localization,Facial feature detection,Feature-based approaches,Image-based approaches},
   pages = {236-274},
   publisher = {Academic Press Inc.},
   title = {Face detection: A survey},
   volume = {83},
   year = {2001},
}


@article{MTCNN,
   abstract = {Face detection and alignment in unconstrained environment are challenging due to various poses, illuminations and occlusions. Recent studies show that deep learning approaches can achieve impressive performance on these two tasks. In this paper, we propose a deep cascaded multi-task framework which exploits the inherent correlation between them to boost up their performance. In particular, our framework adopts a cascaded structure with three stages of carefully designed deep convolutional networks that predict face and landmark location in a coarse-to-fine manner. In addition, in the learning process, we propose a new online hard sample mining strategy that can improve the performance automatically without manual sample selection. Our method achieves superior accuracy over the state-of-the-art techniques on the challenging FDDB and WIDER FACE benchmark for face detection, and AFLW benchmark for face alignment, while keeps real time performance.},
   author = {Kaipeng Zhang and Zhanpeng Zhang and Zhifeng Li and Yu Qiao},
   doi = {10.1109/LSP.2016.2603342},
   issue = {10},
   journal = {IEEE Signal Processing Letters},
   keywords = {Cascaded convolutional neural network (CNN),face alignment,face detection},
   month = {4},
   pages = {1499-1503},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks},
   volume = {23},
   url = {http://arxiv.org/abs/1604.02878 http://dx.doi.org/10.1109/LSP.2016.2603342},
   year = {2016},
}

@article{Young1987,
   abstract = {A new facial composites technique is demonstrated, in which photographs of the top and bottom halves of different familiar faces fuse to form unfamiliar faces when aligned with each other. The perception of a novel configuration in such composite stimuli is sufficiently convincing to interfere with identification of the constituent parts (experiment 1), but this effect disappears when stimuli are inverted (experiment 2). Difficulty in identifying the parts of upright composites is found even for stimuli made from parts of unfamiliar faces that have only ever been encountered as face fragments (experiment 3). An equivalent effect is found for composites made from internal and external facial features of well-known people (experiment 4). These findings demonstrate the importance of configurational information in face perception, and that configurations are only properly perceived in upright faces.},
   author = {A. W. Young and D. Hellawell and D. C. Hay},
   doi = {10.1068/P160747},
   issn = {0301-0066},
   issue = {6},
   journal = {Perception},
   keywords = {A W Young,Adult,Attention*,D C Hay,D Hellawell,Discrimination Learning,Face,Female,Form Perception*,Humans,MEDLINE,Male,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Pattern Recognition,PubMed Abstract,Reaction Time,Visual*,doi:10.1068/p160747,pmid:3454432},
   pages = {747-759},
   pmid = {3454432},
   publisher = {Perception},
   title = {Configurational information in face perception},
   volume = {16},
   url = {https://pubmed.ncbi.nlm.nih.gov/3454432/},
   year = {1987},
}

@article{Andrews2010,
   abstract = {The perception and recognition of familiar faces depends critically on an analysis of the internal features of the face (eyes, nose, mouth). We therefore contrasted how information about the internal and external (hair, chin, face outline) features of familiar and unfamiliar faces is represented in face-selective regions. There was a significant response to both the internal and external features of the face when presented in isolation. However, the response to the internal features was greater than the response to the external features. There was significant adaptation to repeated images of either the internal or external features of the face in the fusiform face area (FFA). However, the magnitude of this adaptation was greater for the internal features of familiar faces. Next, we asked whether the internal features of the face are represented independently from the external features. There was a release from adaptation in the FFA to composite images in which the internal features were varied but the external features were unchanged, or when the internal features were unchanged but the external features varied, demonstrating a holistic response. Finally, we asked whether the holistic response to faces could be influenced by the context in which the face was presented. We found that adaptation was still evident to composite images in which the face was unchanged but body features were varied. Together, these findings show that although internal features are important in the neural representation of familiar faces, the face's internal and external features are represented holistically in face-selective regions of the human brain.},
   author = {Timothy J. Andrews and Jodie Davies-Thompson and Alan Kingstone and Andrew W. Young},
   doi = {10.1523/JNEUROSCI.4863-09.2010},
   issn = {0270-6474},
   issue = {9},
   journal = {Journal of Neuroscience},
   month = {3},
   pages = {3544-3552},
   pmid = {20203214},
   publisher = {Society for Neuroscience},
   title = {Internal and External Features of the Face Are Represented Holistically in Face-Selective Regions of Visual Cortex},
   volume = {30},
   url = {https://www.jneurosci.org/content/30/9/3544 https://www.jneurosci.org/content/30/9/3544.abstract},
   year = {2010},
}

@newspaper_article{Birchall2017,
   author = {Guy Birchall and Tom Michael and The Sun},
   journal = {Chinese users claim iPhone X face recognition can’t tell them apart},
   month = {12},
   title = {Chinese users claim iPhone X face recognition can’t tell them apart},
   url = {https://nypost.com/2017/12/21/chinese-users-claim-iphone-x-face-recognition-cant-tell-them-apart/},
   year = {2017},
}

@article{NMI,
   abstract = {Given the increasing popularity of algorithms for overlapping clustering, in
particular in social network analysis, quantitative measures are needed to
measure the accuracy of a method. Given a set of true clusters, and the set of
clusters found by an algorithm, these sets of clusters must be compared to see
how similar or different the sets are. A normalized measure is desirable in
many contexts, for example assigning a value of 0 where the two sets are
totally dissimilar, and 1 where they are identical. A measure based on
normalized mutual information, [1], has recently become popular. We demonstrate
unintuitive behaviour of this measure, and show how this can be corrected by
using a more conventional normalization. We compare the results to that of
other measures, such as the Omega index [2].},
   author = {Aaron F. McDaid and Derek Greene and Neil Hurley},
   doi = {10.48550/arxiv.1110.2515},
   month = {10},
   title = {Normalized Mutual Information to evaluate overlapping community finding algorithms},
   url = {https://arxiv.org/abs/1110.2515v2},
   year = {2011},
}

@article{Golalipour2021,
   abstract = {Clustering, as an unsupervised learning, is aimed at discovering the natural groupings of a set of patterns, points, or objects. In clustering algorithms, a significant problem is the absence of a deterministic approach based on which users can decide which clustering method best matches a given set of input data. This is due to using certain criteria for optimization. Clustering ensemble as a knowledge reuse offers a solution to solve the challenges inherent in clustering. It seeks to explore results of high stability and robustness by composing computed solutions achieved by base clustering algorithms without getting access to the features. Combining base clusterings together degrades the quality of the final solution when low-quality ensemble members are used. Several researchers in this field have suggested the concept of clustering ensemble selection for the aim of selecting a subset of base clustering based on quality and diversity. While clustering ensemble makes a combination of all ensemble members, clustering ensemble selection chooses a subset of ensemble members and forms a smaller cluster ensemble that performs better than the clustering ensemble. This survey includes the historical development of data clustering that makes an overview on basic clustering techniques, discusses clustering ensemble algorithms including ensemble generation mechanisms and consensus function, and point out clustering ensemble selection techniques with considering quality and diversity.},
   author = {Keyvan Golalipour and Ebrahim Akbari and Seyed Saeed Hamidi and Malrey Lee and Rasul Enayatifar},
   doi = {10.1016/J.ENGAPPAI.2021.104388},
   issn = {0952-1976},
   journal = {Engineering Applications of Artificial Intelligence},
   keywords = {Cluster analysis,Clustering ensemble,Clustering ensemble selection,Consensus clustering,Data clustering},
   month = {9},
   pages = {104388},
   publisher = {Pergamon},
   title = {From clustering to clustering ensemble selection: A review},
   volume = {104},
   year = {2021},
}

@software{Sano_ClusterEnsembles_2021,
author = {Sano, Takehiro},
license = {MIT},
month = {8},
title = {{ClusterEnsembles}},
url = {https://github.com/tsano430/ClusterEnsembles},
version = {1.0.0},
year = {2021}
}

@article{Fern2004,
   abstract = {A critical problem in cluster ensemble research is how to combine multiple clusterings to yield a final superior clustering result. Leveraging advanced graph partitioning techniques, we solve this problem by reducing it to a graph partitioning problem. We introduce a new reduction method that constructs a bipartite graph from a given cluster ensemble. The resulting graph models both instances and clusters of the ensemble simultaneously as vertices in the graph. Our approach retains all of the information provided by a given ensemble, allowing the similarity among instances and the similarity among clusters to be considered collectively in forming the final clustering. Further, the resulting graph partitioning problem can be solved efficiently. We empirically evaluate the proposed approach against two commonly used graph formulations and show that it is more robust and achieves comparable or better performance in comparison to its competitors.},
   author = {Xiaoli Zhang Fern and Carla E. Brodley},
   doi = {10.1145/1015330.1015414},
   isbn = {1581138385},
   journal = {Proceedings, Twenty-First International Conference on Machine Learning, ICML 2004},
   pages = {281-288},
   title = {Solving cluster ensemble problems by bipartite graph partitioning},
   year = {2004},
}

@article{MVLearn,
  title={mvlearn: Multiview Machine Learning in Python},
  author={Perry, Ronan and Mischler, Gavin and Guo, Richard and Lee, Theodore and Chang, Alexander and Koul, Arman and Franz, Cameron and Richard, Hugo and Carmichael, Iain and Ablin, Pierre and Gramfort, Alexandre and Vogelstein, Joshua T.},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={109},
  pages={1-7},
  year={2021}
}

@Article{YaleDataset,
  author =  "Georghiades, A.S. and Belhumeur, P.N. and Kriegman, D.J.",
  title =   "From Few to Many: Illumination Cone Models for Face Recognition under
               Variable Lighting and Pose",
  journal = "IEEE Trans. Pattern Anal. Mach. Intelligence",
  year =  2001,
  volume = 23,
  number = 6,
  pages= "643-660"}

@inproceedings{CELEBA,
  title = {Deep Learning Face Attributes in the Wild},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
  month = {December},
  year = {2015} 
}

@TechReport{LFW,
  author =       {Gary B. Huang and Manu Ramesh and Tamara Berg and 
                  Erik Learned-Miller},
  title =        {Labeled Faces in the Wild: A Database for Studying 
                  Face Recognition in Unconstrained Environments},
  institution =  {University of Massachusetts, Amherst},
  year =         2007,
  number =       {07-49},
  month =        {October}}

@article{MLFW,
  title={MLFW: A Database for Face Recognition on Masked Faces}, 
  author={Wang, Chengrui and Fang, Han and Zhong, Yaoyao and Deng, Weihong},
  journal={arXiv preprint arXiv:2109.05804},
  year={2021}
}

@article{JULE,
   abstract = {In this paper, we propose a recurrent framework for Joint Unsupervised
LEarning (JULE) of deep representations and image clusters. In our framework,
successive operations in a clustering algorithm are expressed as steps in a
recurrent process, stacked on top of representations output by a Convolutional
Neural Network (CNN). During training, image clusters and representations are
updated jointly: image clustering is conducted in the forward pass, while
representation learning in the backward pass. Our key idea behind this
framework is that good representations are beneficial to image clustering and
clustering results provide supervisory signals to representation learning. By
integrating two processes into a single model with a unified weighted triplet
loss and optimizing it end-to-end, we can obtain not only more powerful
representations, but also more precise image clusters. Extensive experiments
show that our method outperforms the state-of-the-art on image clustering
across a variety of image datasets. Moreover, the learned representations
generalize well when transferred to other tasks.},
   author = {Jianwei Yang and Devi Parikh and Dhruv Batra},
   doi = {10.48550/arxiv.1604.03628},
   isbn = {9781467388504},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   month = {4},
   pages = {5147-5156},
   publisher = {IEEE Computer Society},
   title = {Joint Unsupervised Learning of Deep Representations and Image Clusters},
   volume = {2016-December},
   url = {https://arxiv.org/abs/1604.03628v3},
   year = {2016},
}

